{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b533d0c1",
   "metadata": {},
   "source": [
    "## Seminar 7. Modern LLM\n",
    "\n",
    "### Plan\n",
    "\n",
    "1. Modern language models.\n",
    "2. The difference between pre-trained models and fine-tuned models using Llama 3.2-1B as an example:   \n",
    "    - Architecture \n",
    "    - Tokenizers \n",
    "    - Chat Template\n",
    "3. Supervised finetuning\n",
    "4. RLHF \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25345ab0",
   "metadata": {},
   "source": [
    "## Modern LLM\n",
    "\n",
    "All Instruct models are now undergoing three stages of training: pre-training, supervised fine-tuning and RLHF. Let's go through each of them.\n",
    "\n",
    "### Pre-training\n",
    "Language models are trained on the task of Next token prediction. For this purpose, huge datasets are usually collected automatically. Standard data sources: parsing web sites (with subsequent cleaning from html tags and other stuff), parsing book collections and so on. Examples of datasets: [c4](https://huggingface.co/datasets/allenai/c4), [RedPajama](https://github.com/togethercomputer/RedPajama-Data), [wikipedia](https://huggingface.co/datasets/wikimedia/wikipedia)\n",
    "\n",
    "There are many benchmarks available to test LLM abilities, such as, [hellaswag](https://arxiv.org/abs/1905.07830), [OpenBookQA](https://huggingface.co/datasets/allenai/openbookqa), [WinoGrande](https://huggingface.co/datasets/allenai/winogrande). These benchmarks most often ask for either a Multiple-Choice response or a continuation response. \n",
    "\n",
    "LLMs have excellent knowledge of the world due to the fact that they have seen a lot of data, but they have a problem: they don't know how to respond in a format that is useful to humans. Let's look at the example of Mistral-7B-v0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5978a479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a537ff7a",
   "metadata": {},
   "source": [
    "Let's try asking her to solve a simple problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffa970c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aed1bc1a6f041a0a33dbd664f898e62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "pipe_pretrained = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-v0.1\", max_new_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12f0b014",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"Find the value of n in n + 2 = 6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c2e41da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Find the value of n in n + 2 = 6\\n\\n#### Solution\\n\\nn + 2 = 6\\n\\nn = 6 - 2\\n\\nn = 4\\n\\nConcept: Introduction of Algebra\\n  Is there an error in this question or solution?\\n\\n#### Video TutorialsVIEW ALL [1]\\n\\n- view\\nVideo Tutorials For All Subjects\\n- Introduction of Algebra\\n\\nvideo tutorial00:07:55'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_pretrained(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abf2811",
   "metadata": {},
   "source": [
    "Something has clearly gone wrong. In fact, it is well solved by making the prompt in the form of a continuation of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de8e0363",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_fixed = \"Find the value of n in n + 2 = 6. Answer: n = \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cea8ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Find the value of n in n + 2 = 6. Answer: n = 4.\\n\\n## What is the value of n in n + 2 = 6?\\n\\nThe value of n in n + 2 = 6 is 4.\\n\\n## How do you solve n + 2 = 6?\\n\\nTo solve the equation n + 2 = 6, we need to subtract 2 from both sides of the equation. This will give us the equation n = 4.\\n\\n## What is the value of n in n + 2 = 6?\\n\\nThe value of n in n + 2 = 6 is 4.\\n\\n## How do you solve n + 2 = 6?\\n\\nTo solve the equation n + 2 = 6, we need to subtract 2 from both sides of the equation. This will give us the equation n = 4.\\n\\n## What is the value of n in n + 2 = 6?\\n\\nThe value of n in n + 2 = 6 is 4.\\n\\n## How do you solve n + 2 = 6?\\n\\nTo solve the equation n + 2 = 6, we need to subtract 2 from both sides of the equation. This will give us the equation n = 4.\\n\\n## What is the value of n in n + 2 = 6?\\n\\nThe value of n in n + 2 = 6 is 4.\\n\\n## How do you solve n + 2 = 6?\\n\\nTo solve the equation n + 2 = 6, we need to subtract 2 from both sides of the equation. This will give us the equation n = 4.\\n\\n## What is the value of n in n + 2 = 6?\\n\\nThe value of n in n + 2 = 6 is 4.\\n\\n## How do you solve n + 2 = 6?\\n\\nTo solve the equation n + 2 = 6, we need to subtract 2 from both sides of the equation. This will give us the equation n = 4.\\n\\n## What is the value of n in n + 2 = 6?\\n\\nThe value of n in n + 2 = 6 is 4.\\n\\n## How do you solve n + 2 = 6?\\n\\nTo solve the equation n + '}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_pretrained(task_fixed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552e9aaf",
   "metadata": {},
   "source": [
    "It's already better, but it's far from the way ChatGPT behaves. What's the reason? \n",
    "\n",
    "**Models that are designed for user interaction go through several more stages of learning. Generally these stages are called Alignment (although everyone has different meanings for this term nowadays)**\n",
    "\n",
    "The name comes from the OpenAI paper [Aligning language models to follow instructions](https://openai.com/index/instruction-following/).\n",
    "\n",
    "Let's see how the chat model handles such tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4651594f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebd382932ba748ec974e8166c6bfda38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-Instruct-v0.1\", max_new_tokens=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c94c90",
   "metadata": {},
   "source": [
    "We'll have to change the input data format a bit, since chat models accept data in dialog format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02da7004",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Find the value of n in n + 2 = 6\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb430cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': [{'role': 'user',\n",
       "    'content': 'Find the value of n in n + 2 = 6'},\n",
       "   {'role': 'assistant',\n",
       "    'content': ' To find the value of n in the equation n + 2 = 6, we need to isolate n on one side of the equation. \\n\\nWe can start by subtracting 2 from both sides of the equation: \\n\\nn + 2 - 2 = 6 - 2 \\n\\nThis simplifies to: \\n\\nn = 4 \\n\\nTherefore, the value of n in the equation n + 2 = 6 is 4.'}]}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ad6705",
   "metadata": {},
   "source": [
    "Here we see the kind of response we are used to in ChatGPT. The most interesting thing is that there are no differences in the models themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a091cb43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33d5dfdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_pretrained.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22184af",
   "metadata": {},
   "source": [
    "## How is such a difference in responses achieved?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd476fc1",
   "metadata": {},
   "source": [
    "## Supervised Fine-Tuning\n",
    "\n",
    "How are people most often taught to do something? They are given some task and shown a good solution to that task. \n",
    "\n",
    "Here the idea is the same: let's collect a set of data in the format of a dialog between a user and an assistant. The user asks a question, the assistant answers it. \n",
    "\n",
    "Ideally (as was done in the original article) the answers should be written by people who are experts in the domain. \n",
    "\n",
    "Thus, the model learns the answer format and uses its knowledge from the previous stage to help the user.\n",
    "\n",
    "Examples of such datasets with instructions are: [UltraChat](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k), [No-Robots](https://huggingface.co/datasets/HuggingFaceH4/no_robots), [MathInstruct](https://huggingface.co/datasets/TIGER-Lab/MathInstruct). \n",
    "\n",
    "To collect such datasets, large companies (OpenAI, Meta, Google, etc.) have assembled entire departments consisting of professional editors to write instructions and answers to them. \n",
    "\n",
    "Then, standard training is performed on the obtained data as in the previous stage (next-token prediction)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be5229f",
   "metadata": {},
   "source": [
    "![SFT](https://klu.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fklu-supervised-fine-tuning-sft.ca014122.png&w=3840&q=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5933d391",
   "metadata": {},
   "source": [
    "### In fact, it is now possible to collect such data without expensive markup editors.\n",
    "\n",
    "There are several approaches, such as [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html), that allow you to collect data automatically. \n",
    "\n",
    "Alpaca is organized as follows: \n",
    "\n",
    "- Select 175 human-written instructions from the self-instruct dataset\n",
    "- Prompt GPT-3.5 (which was SOTA at the time) to generate more similar instructions.\n",
    "- Learn from the data\n",
    "\n",
    "![Alpaca](https://crfm.stanford.edu/static/img/posts/2023-03-13-alpaca/alpaca_main.jpg)\n",
    "\n",
    "But there are other methods, such as [Instruction Pre-Training](https://arxiv.org/abs/2406.14491), [Mammoth](https://arxiv.org/abs/2405.03548)\n",
    "\n",
    "They boil down to the fact that instructions can be generated from the same data on which the model is pre-trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59685242",
   "metadata": {},
   "source": [
    "## Preference Learning (RLHF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f54dc03",
   "metadata": {},
   "source": [
    "But that's not all. Even though SFT already gives good quality, the real breakthrough is the RLHF method, which was first successfully applied by OpenAI in [InstructGPT](https://openai.com/index/instruction-following/)\n",
    "\n",
    "This method proposes to raise not only the probabilities for “good” answers, but also to lower the probabilities for “bad” answers, e.g. - toxic, dangerous or false. \n",
    "\n",
    "What is the essence of the approach:\n",
    "\n",
    "- Multiple answers of the same model are generated for a single query, e.g., by changing the generation parameters.\n",
    "- The answers are ranked by humans: so, an answer without all the “kinks” will be more preferable than one with them\n",
    "- Then, a reward model is trained on the answers, which gives a larger reward to the more “good” answer\n",
    "- After that, Reinforcement Learning techniques, namely PPO, are used to maximize the reward.\n",
    "\n",
    "![RLHF](https://images.ctfassets.net/kftzwdyauwt9/12CHOYcRkqSuwzxRp46fZD/928a06fd1dae351a8edcf6c82fbda72e/Methods_Diagram_light_mode.jpg?w=3840&q=80&fm=webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063f9198",
   "metadata": {},
   "source": [
    "Люди предпочитали ответы меньшей модели, обученной с RLHF, ответам бОльшей модели.\n",
    "\n",
    "![img](https://gcdnb.pbrd.co/images/gi3riropQvQe.png?o=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2794baaa",
   "metadata": {},
   "source": [
    "# DPO\n",
    "\n",
    "\n",
    "Но в то же время PPO это очень дорогой метод, который имеет много гиперпараметров. В его реализации очень легко допустить ошибки, которые тяжело раздебагать. \n",
    "\n",
    "В 2023 году вышла статья [Direct Preference Optimization](https://arxiv.org/abs/2305.18290)\n",
    "\n",
    "Путем сложной матеши авторы вывели, что оптимальная политика выражается через саму функцию (поэтому статья называется Your Language Model is Secretly a Reward model), поэтому можно избежать проблем с PPO и с RL'ем в целом. \n",
    "\n",
    "![DPO](https://ghost.oxen.ai/content/images/2024/01/Screenshot-2024-01-26-at-3.49.01-PM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7724158b",
   "metadata": {},
   "source": [
    "Все эти (и другие) методы доступны в библиотеке [TRL](https://huggingface.co/docs/trl/en/index) \n",
    "\n",
    "- [SFT Trainer](https://huggingface.co/docs/trl/en/sft_trainer) \n",
    "\n",
    "- [Reward Modelling](https://huggingface.co/docs/trl/en/reward_trainer)\n",
    "\n",
    "- [PPO Trainer](https://huggingface.co/docs/trl/en/ppo_trainer)\n",
    "\n",
    "- [DPO Trainer](https://huggingface.co/docs/trl/en/dpo_trainer)\n",
    "\n",
    "![TRL](https://gcdnb.pbrd.co/images/pQ7aghF2128t.png?o=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647a05ac",
   "metadata": {},
   "source": [
    "## Как оценить качество модели и понять, какая нужна именно вам?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cc6dcc",
   "metadata": {},
   "source": [
    "## Существует множество бенчмарков, в которых проверяются определенные навыки моделей. \n",
    "\n",
    "Примеры бенчей: \n",
    "\n",
    "- [MT-Bench](https://arxiv.org/abs/2306.05685): на многоступенчатые диалоги (несколько переходов человек-ассистент) генерируются продолжения, а затем, оцениваются с помощью GPT4.\n",
    "- [AlpacaEval](https://tatsu-lab.github.io/alpaca_eval/): сравнивают ответ модели с ответом ГПТ4, затем оценивают с помощью другой модели. Затем считают win-rate. \n",
    "\n",
    "\n",
    "- [ChatBot Arena](https://lmarena.ai): Пользователям дают возможность пообщаться с разными моделями (в том числе с платными) и задать им свои вопросы. Затем, дают пользователю выбрать тот ответ, что ему больше нравится из двух. На основе этого строят рейтинг моделей. \n",
    "\n",
    "- [LLM Arena](https://llmarena.ru): То же самое, но на русском языке."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f9265f",
   "metadata": {},
   "source": [
    "Текущий лидерборд на арене"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734e8078",
   "metadata": {},
   "source": [
    "![Arena](https://gcdnb.pbrd.co/images/9FoqK20pMVoq.png?o=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d2252a",
   "metadata": {},
   "source": [
    "## Какую модель выбрать?\n",
    "\n",
    "### Почти всегда предпочтение стоит отдавать моделям, которые обучены отвечать в чат-формате. То есть, которые прошли alignment стадию. \n",
    "\n",
    "Учить свою модель с нуля почти всегда не выгодно финансово, а также нецелесообразно, если, конечно, вы не работаете в огромной компании, у которой есть ресурсы на это. \n",
    "\n",
    "\n",
    "Современные модели относительно \"маленького\" размера ([gemma2 9b](https://huggingface.co/google/gemma-2-9b-it), [llama-3.1 8B](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)) неплохо справляются с общими задачами, на порядок лучше, чем \"старые\" модели даже больших размеров.\n",
    "\n",
    "Большие модели ([gemma2 27b](https://huggingface.co/google/gemma-2-27b-it), [llama3.1 405b](https://huggingface.co/meta-llama/Llama-3.1-405B-Instruct)) зачастую даже не нужно дополнительно обучать под конкретные задачи (например, суммаризация или классификация), достаточно грамотно написать задачу модели и описать какой формат выходных данных от нее ожидается. \n",
    "\n",
    "Подробнее про prompt-engineering можно почитать [тут](https://platform.openai.com/docs/guides/prompt-engineering)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
