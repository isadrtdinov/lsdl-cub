{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa6c9d7e-8f48-41e9-94ab-5c3b1b717694",
   "metadata": {},
   "source": [
    "# LSDL CUB, Homework 4. Model size reduction [10 pts]\n",
    "\n",
    "__Soft deadline 29.11.24 23:59__ \\\n",
    "__Hard deadline 2.12.24 23:59__\n",
    "\n",
    "### About this task\n",
    "\n",
    "In this task you will learn how to solve the Named Entity Recognition (NER) problem on the most popular dataset - [CoNLL-2003](https://paperswithcode.com/dataset/conll-2003). You will have a pre-trained BERT at your disposal, which you need to reduce with minimal loss in quality to the size of 20M parameters. To do this you will implement embedding factorisation, knowledge distillation, parameter sharing and so on.\n",
    "\n",
    "In this assignment you will have to do quite a lot of experiments, so we recommend you not to write all the code in a notebook, but to make different files for separate logical blocks and compose everything in the form of a project. This will keep your notebook from getting too messy and will make the task much easier for both you and the graders. Also try to log all your experiments in wandb so that nothing gets lost.\n",
    "\n",
    "### Grading\n",
    "\n",
    "The grade for this homework assignment will be made up of a grade for __tasks__ and a __report__, in which, as before, you are required to write about the work you have done. You may receive up to 2 points for the report, but you may lose points for the tasks themselves if you do not write a report. The assignments are divided into two parts: _numerical_ and _of choice_. For _numerical_ you can get a total of 6 points, for _of choice_ assignments you can get up to 16 points. That is, you can get 24 points for the assignment. Anything you score above 10 will be considered bonuses.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "Named Entity Recognition is the task of classifying tokens into entity classes. CoNLL-2003 uses **BIO** (Beggining, Inside, Outside) tagging to name entities, where the tags mean the following:\n",
    "\n",
    "- *B-{entity}* - the beginning of the entity *{entity}*\n",
    "- *I-{entity}* - the continuation of the entity *{entity}*\n",
    "- *O* - not an entity.\n",
    "\n",
    "There are other ways of labelling, such as BILUO. You can read about them [here](https://en.wikipedia.org/wiki/Inside-outside-beginning_(tagging)).\n",
    "\n",
    "There are a total of 9 different tags in the dataset.\n",
    "- O - no entity corresponds to the word.\n",
    "- B-PER/I-PER - the word or set of words corresponds to a particular _person_.\n",
    "- B-ORG/I-ORG - the word or set of words corresponds to a specific _organisation_.\n",
    "- B-LOC/I-LOC - a word or set of words corresponds to a particular _location_.\n",
    "- B-MISC/I-MISC - a word or set of words corresponds to an entity that does not belong to any of the previous ones. For example, a nationality, a work of art, an event, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe56a70-a72d-40a0-9ae3-395ec6460657",
   "metadata": {},
   "source": [
    "Let's start with loading and preprocessing the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ef87648-86ab-4f81-9db3-5cb7f54c575d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"eriktks/conll2003\")\n",
    "\n",
    "dataset = dataset.remove_columns([\"id\", \"pos_tags\", \"chunk_tags\"])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7c1a5b0-6ca1-4159-9ce6-cff88aca6b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['EU',\n",
       "  'rejects',\n",
       "  'German',\n",
       "  'call',\n",
       "  'to',\n",
       "  'boycott',\n",
       "  'British',\n",
       "  'lamb',\n",
       "  '.'],\n",
       " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04b0e34d-edca-40bc-83ac-cff0c2872f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "958cdff9-6ea1-4f7f-808b-dbe5620c27e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EU\tB-ORG\n",
      "rejects\tO\n",
      "German\tB-MISC\n",
      "call\tO\n",
      "to\tO\n",
      "boycott\tO\n",
      "British\tB-MISC\n",
      "lamb\tO\n",
      ".\tO\n"
     ]
    }
   ],
   "source": [
    "words = dataset[\"train\"][0][\"tokens\"]\n",
    "labels = dataset[\"train\"][0][\"ner_tags\"]\n",
    "\n",
    "for i in range(len(words)):\n",
    "    print(f'{words[i]}\\t{label_names[labels[i]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef2312e-1335-4afa-a6e6-3cdde8515fe5",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Throughout this homework we will be using the _cased_ version of BERT, meaning that the tokeniser will take case of words into account. For the NER task, case is important because names and organisations or art objects are often capitalised and it would be silly to hide such information from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05edcd4e-5360-41a8-b403-a9084d6a3a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f64076-829a-49f1-af58-6fe60c66f965",
   "metadata": {},
   "source": [
    "During tokenisation, words can be split into several tokens (like the word `Fischler` from the example below), causing a mismatch between the number of tokens and labels. We will have to fix this mismatch manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ebc8789-0bba-4c96-aa1a-84403c93260e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words:  ['Only', 'France', 'and', 'Britain', 'backed', 'Fischler', \"'s\", 'proposal', '.']\n",
      "Tokens: ['[CLS]', 'Only', 'France', 'and', 'Britain', 'backed', 'Fi', '##sch', '##ler', \"'\", 's', 'proposal', '.', '[SEP]']\n",
      "Tags: ['O', 'B-LOC', 'O', 'B-LOC', 'O', 'B-PER', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "example = dataset[\"train\"][12]\n",
    "words = example[\"tokens\"]\n",
    "tags = [label_names[t] for t in example[\"ner_tags\"]]\n",
    "tokenized_text = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "\n",
    "print('Words: ', words)\n",
    "print('Tokens:', tokenized_text.tokens())\n",
    "print('Tags:', tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34949bff-d7e9-47b3-aea7-82f1332a899c",
   "metadata": {},
   "source": [
    "__Task 1 [1 pts].__ Tokenise the entire dataset and for each text, align tokens with labels so that each token corresponds to one label. It is important to preserve the BIO notation while doing this. And don't forget the special tokens! You should get something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8352b8f-f60a-4844-b428-9e866678dc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned tags: [-100    0    5    0    5    0    1    2    2    0    0    0    0 -100]\n",
      "Aligned named tags: [-100, 'O', 'B-LOC', 'O', 'B-LOC', 'O', 'B-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', -100]\n"
     ]
    }
   ],
   "source": [
    "aligned_labels = align_labels_with_tokens(example[\"ner_tags\"], tokenized_text)\n",
    "tags = [label_names[t] if t > -1 else t for t in aligned_labels]\n",
    "print(\"Aligned tags:\", aligned_labels)\n",
    "print(\"Aligned named tags:\", tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93809cc7-4cde-4764-ab2d-6d9a76f879e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354e9399-8393-4312-88aa-53b727d9df7d",
   "metadata": {},
   "source": [
    "### Metric.\n",
    "\n",
    "The F1 measure with micro-averaging is commonly used to evaluate the quality of NER. We will load it from the `seqeval` library. The `f1_score` function takes two 2d lists with correct and predicted labels written in text, and returns an F1 value for them. You can use it with default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3df54ab-c65b-40e0-b479-25d6f29e5f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "380833ce-1b8e-4b00-90ee-9126df16c19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc901ecf-2de9-4c3f-862c-cf78871d8d9f",
   "metadata": {},
   "source": [
    "A peculiarity of F1 metric for NER is that in some situations incorrect answers can be counted as correct answers. For example, if the model predicted `[‘I-PER’, ‘I-PER’]`, we can guess that it should actually be `[‘B-PER’, ‘I-PER’]`, since an entity cannot start with `I-`. The `f1_score` function takes this into account and therefore only works with textual representations of labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61400bf-712a-4dfb-a08f-326c5db10eb2",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "We will take `bert-base-cased` as our base model. As you realise, it has not been trained on the NER task. Therefore, it needs to be fine-tuned before proceeding to reduce its size.\n",
    "\n",
    "__Task 2 [1 pts]__ Fine-tune `bert-base-cased` on our dataset using standard fine-tuning. You should get at least 0.9 F1 on the test set. Note that the higher the quality of the large model, the better the distilled student will perform. You can use the `Trainer` from Hugging Face for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14235f7c-d7a3-4407-98fe-be35bec84008",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 107726601\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained('bert-base-cased', num_labels=len(label_names))\n",
    "\n",
    "print('Number of parameters:', sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9ae197eb-041a-4cdb-864d-69b9cf099be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cda5a7-7fb9-43bc-84e3-e66ec6a48d91",
   "metadata": {},
   "source": [
    "### Factorisation of the embedding matrix\n",
    "\n",
    "We can see that at this point the embedding matrix has $V \\cdot H = 28996 \\cdot 768 = 22.268.928$ parameters. That's as much as a fifth of the entire model! Let's try to do something about it. The [ALBERT](https://arxiv.org/pdf/1909.11942.pdf) proposes to factorise the embedding matrix into the product of two smaller matrices. Thus, the embedding parameters will contain $V \\cdot E + E \\cdot H$ elements, which is much smaller than $V \\cdot H$ if $H \\gg E$. The authors choose $E = 128$, but nothing prevents us from taking any other value. For example, by choosing $H = 64$, we reduce the number of parameters by about 20M.\n",
    "\n",
    "__Task 3 [1 pts].__ Write a wrapper class over the embedding layer that implements factorisation into two matrices, and fine-tune the factorised model. Note, both matrices can be initialised using SVD decomposition so that the initial approximation is good. This will save a great amount of time during training. With decomposition rank $H = 64$ you should get an F1 greater than 0.87."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1639dac-cf3c-4312-8330-d4f357f38c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f01e333-e9bd-4bf5-bb84-690a9bf01162",
   "metadata": {},
   "source": [
    "### Knowledge distillation\n",
    "\n",
    "Knowledge distillation is a learning paradigm in which the knowledge of a teacher model is distilled into a student model. The student can be an arbitrary smaller model that solves the same problem, but typically the student has the same architecture as the teacher. Two error functions are used in the distillation:\n",
    "1. Standard cross-entropy.\n",
    "1. A function specifying the distance between the distributions of teacher and student predictions. KL-divergence is most commonly used.\n",
    "\n",
    "To ensure that the teacher's prediction distribution is not degenerate, a temperature greater than 1, such as 2 or 5, is added to softmax.   \n",
    "__Important:__ when dividing logits by temperature, the gradient values decrease by a factor of $\\tau^2$ (check this!). So to return them to their original scale, the error must be multiplied by $\\tau^2$. You can read more about this in section 2.1 [of the original article](https://arxiv.org/pdf/1503.02531).\n",
    "\n",
    "<img src=\"https://intellabs.github.io/distiller/imgs/knowledge_distillation.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450e921f-279e-46ae-8c70-5d715b91106e",
   "metadata": {},
   "source": [
    "__Task 4 [3 pts].__ Implement the knowledge distillation method shown in the picture. Use KL-divergence [`nn.KLDivLoss(reduction=\"batchmean\")`](https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html) (be careful with the format of its inputs) to calculate the loss between student and teacher predictions. Sum the soft loss with the hard loss to get the total loss.   \n",
    "As the teacher, use the pre-trained BERT from task 2. As a student, take the untrained model with a size __no larger than 20M__ parameters. You can use factorisation of the embedding matrix to reduce the number of parameters. If you have done everything correctly, you should get an F1 value of at least 0.7 on the test set. You should be able to do this with about 20k iterations of training. If you fail, you can refer to [DistilBERT](https://arxiv.org/abs/1910.01108) and [this article](https://www.researchgate.net/publication/375758425_Knowledge_Distillation_Scheme_for_Named_Entity_Recognition_Model_Based_on_BERT) for details.\n",
    "\n",
    "__Important:__\n",
    "* Don't forget to add _warmup_ when training a student.\n",
    "* Don't forget to put the teacher in _eval_ mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f571ae8-27d8-4bf8-9e5b-fc5114ec2838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5405767d-ab58-4ec0-a3ac-b117785be7fa",
   "metadata": {},
   "source": [
    "## Tasks to choose from\n",
    "\n",
    "As you realise, there are still quite a few different ways to reduce the size of a trained model. This section asks you to implement different techniques. You can get a different number of points for each one depending on the difficulty. Successful implementation will be judged on both code and quality on a test set. Any points for this assignment that you score above 10 will be considered bonus points.  \n",
    "In task 4, you trained a model with a 20M parameter constraint. When implementing the techniques in this section, stick to the same restriction. This will allow you to fairly compare the techniques against each other and draw the correct conclusions. Write in your report about everything you tried.\n",
    "\n",
    "* __Weights sharing [2 pts].__ The [ALBERT](https://arxiv.org/pdf/1909.11942.pdf) modification of BERT proposes to share weights between layers in addition to embedding factorisation. That is, different layers use the same weights. This technique is equivalent to applying the same layer several times. It allows to reduce the number of parameters several times and not to lose much in quality.\n",
    "* __Factorisation of intermediate layers [2 pts].__ If you can factorise the embedding matrix, you can factorise everything else too. There are many different approaches for factorising layers and it is hard to choose one. You can be inspired by [this list](https://lechnowak.com/posts/neural-network-low-rank-factorization-techniques/), find something else on the internet, or come up with a method on your own. Either way, in the report, justify why you decided to do it the way you did.\n",
    "* __Distillation for intermediate layers (2 points).__ We discussed that in addition to minimizing the distance between the outputs of the student and the teacher, you can minimize the distance between their intermediate layers. [This paper](https://www.researchgate.net/publication/375758425_Knowledge_Distillation_Scheme_for_Named_Entity_Recognition_Model_Based_on_BERT) details how this can be done.\n",
    "* __Pruning (4 points).__ The [SparseGPT](https://arxiv.org/abs/2301.00774) method proposes an approach that removes some of the model weights after training. Turns out that it is possible to remove up to half of all weights without loss in quality. The maths behind the method is quite complex, but the general approach is simple - we will remove weights in each layer separately and when removing part of the layer weights, the remaining weights will be reconfigured so that the overall output of the layer is unchanged.\n",
    "* __Removal of heads (6 points).__ At this point we are using all attention heads, but a number of studies show that most of them can be discarded without loss of quality. This [paper](https://arxiv.org/pdf/1905.09418.pdf) proposes an approach that adds gates to the attention mechanism that regulate which heads participate in the layer and which do not. During training, the gates are adjusted so that most heads are not used. At the end of training, unused heads can be removed. A lot of points are given for this task because the method has quite complex math and it is hard to make the approach work. If you decide to spend your effort on it, we will give intermediate points based on the report if you fail.   \n",
    "__Tip:__ while training a model, watch the behaviour of the gates carefully. If you have done everything correctly, they should zero out. However, they do not always zero out immediately, you should give them time and train the model longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0d7fb6-3983-4509-942e-59edef236eb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
